{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "from tensorflow.keras.layers import Input, Dense, Concatenate, Subtract, ReLU\nfrom tensorflow.keras.layers import Lambda, Add, Dot, BatchNormalization, Activation\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.initializers import he_normal, Zeros, he_uniform, TruncatedNormal\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nimport numpy as np\n\n# BN -> batch normalization\n# d -> number of hidden layers - including input layer\n# m -> number of neurons in each hidden layers\n\nclass StratLayer(tf.keras.layers.Layer):\n    \n    def __init__(self, d = None, m = None, use_batch_norm = None, \\\n                 kernel_initializer = he_uniform(), activation_dense = \"relu\", \\\n                 activation_output = \"linear\", day = None):\n        \n        super().__init__(name = \"delta_\" + str(day))\n        self.d = d\n        self.m = m\n        self.use_batch_norm = use_batch_norm\n        self.activation_dense = activation_dense\n        self.activation_output = activation_output\n        self.kernel_initializer = kernel_initializer\n        \n        self.intermediate_dense = [None for _ in range(d)]\n        self.intermediate_BN = [None for _ in range(d)]\n        \n        for i in range(d):\n            self.intermediate_dense[i] = Dense(self.m, kernel_initializer = self.kernel_initializer, \\\n                                               bias_initializer = he_uniform(), \\\n                                               use_bias = (not self.use_batch_norm))\n            if self.use_batch_norm:\n                self.intermediate_BN[i] = BatchNormalization(momentum = 0.99, trainable = True)\n        \n        self.output_dense = Dense(1, kernel_initializer = self.kernel_initializer, \\\n                                  bias_initializer = he_uniform(), use_bias = True)\n        \n    def call(self, input):\n        \n        for i in range(self.d):\n            if i == 0:\n                output = self.intermediate_dense[i](input)\n            else:\n                output = self.intermediate_dense[i](output)\n                \n            if self.use_batch_norm:\n                output = self.intermediate_BN[i](output, training = True)\n                \n            if self.activation_dense == \"relu\":\n                output = ReLU()(output)\n            else:\n                output = Activation(self.activation_dense)(output)\n            \n        output = self.output_dense(output)\n        \n        if self.activation_output == \"relu\":\n            output = ReLU()(output)\n        elif self.activation_output == \"tanh\":\n            output = Activation(self.activation_output)(output)\n        \n        return output",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}